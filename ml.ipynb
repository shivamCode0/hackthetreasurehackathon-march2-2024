{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Workshop: Generating Text using an N-gram Model\n",
    "\n",
    "## What is an N-gram Model?\n",
    "\n",
    "An $N$-gram model is a simple but powerful tool used in natural language processing (NLP) for predicting the next word in a sequence of words. It's based on the idea that the probability of a word occurring depends on the previous $N-1$ words in the sequence.\n",
    "\n",
    "## How Does it Work?\n",
    "\n",
    "Imagine we have a sentence: \"The quick brown fox jumps\". \n",
    "\n",
    "- A unigram model ($N=1$) predicts the next word based on the probability of individual words. For example, given \"The\", it might predict \"quick\" with high probability because \"quick\" often follows \"The\".\n",
    "- A bigram model ($N=2$) predicts the next word based on the probability of pairs of words. For example, given \"quick brown\", it might predict \"fox\" because \"fox\" often follows \"quick brown\".\n",
    "- An $N$-gram model generalizes this concept to predict the next word based on the probability of the previous $N-1$ words.\n",
    "\n",
    "## Model Architecture\n",
    "\n",
    "In our workshop, we'll create a simple $N$-gram model using PyTorch. Here's a simplified overview of the architecture:\n",
    "\n",
    "1. **Tokenization**: We'll convert our text data into tokens (words or characters).\n",
    "2. **Embedding**: Each token is represented as a high-dimensional vector (embedding).\n",
    "3. **Concatenation**: We concatenate the embeddings of the previous $N-1$ tokens into a single vector.\n",
    "4. **Fully-Connected Layers**: We pass the concatenated vector through fully-connected layers with activation functions like ReLU.\n",
    "5. **Output Layer**: Finally, we predict the probability distribution over the vocabulary to determine the next token.\n",
    "\n",
    "## Workshop Goals\n",
    "\n",
    "By the end of this workshop, you'll understand the basics of $N$-gram models and how they can be implemented using PyTorch. You'll also be able to train your own model and generate text based on the patterns it learns from the training data.\n",
    "\n",
    "Let's dive in and explore the world of NLP with $N$-gram models!\n",
    "\n",
    "---\n",
    "\n",
    "*Author: Shivam Gupta*\n",
    "\n",
    "*GitHub Repository: [shivamCode0/ngram](https://github.com/shivamCode0/ngram)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Setup (Run this block first)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torchinfo\n",
    "%pip install torchviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n",
    "from torchviz import make_dot\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download file to tmp/data.txt\n",
    "!wget -O data.txt https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load & Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "print('text length:', len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️⚠️⚠️ Print first 100 characters of text ⚠️⚠️⚠️\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: print first 100 characters of text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = [\"[PAD]\", *sorted(list(set(text)))]\n",
    "vocab_size = len(chars)\n",
    "print(\"\".join(chars))\n",
    "print(\"vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mapping from character to index and vice versa\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️⚠️⚠️ Verify that `encode` and `decode` work ⚠️⚠️⚠️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decode(encode(\"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store in tensor\n",
    "data = torch.tensor(encode(text), dtype=torch.int64, device=device)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100]) # the 100 characters we looked at earier will to the GPT look like this\n",
    "# Let's now split up the data into train and validation sets\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "block_size = 100\n",
    "train_data[:block_size+1]\n",
    "a = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "torch.manual_seed(1337)\n",
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "block_size = 10 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape, xb.dtype, xb.device)\n",
    "print('targets:')\n",
    "print(yb.shape, yb.dtype, yb.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseNGram(nn.Module, ABC):\n",
    "    def __init__(self, vocab_size, n):\n",
    "        super().__init__()\n",
    "        super().to(device)\n",
    "        assert n >= 3, \"n should be at least 3\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def run_calculations(self, x):\n",
    "        # to be implemented in child classes\n",
    "        pass\n",
    "    \n",
    "    # Create separate function for forward calculation\n",
    "    def forward(self, x, only_last=False):\n",
    "        assert len(x.shape) == 2, \"input shape should be (batch, time)\"\n",
    "\n",
    "        if only_last:\n",
    "            # pad time dim to at least n\n",
    "            x = x[:, -self.n :]\n",
    "            x = F.pad(x, (self.n - x.shape[1], 0), value=0)\n",
    "            B, N = x.shape\n",
    "            x = x.view(B, 1, N)\n",
    "        else:\n",
    "            new_x = torch.zeros((x.shape[0], x.shape[1], self.n), dtype=torch.int64, device=device) - 69\n",
    "            for time_index in range(x.shape[1]):\n",
    "                row = x[:, max(0, time_index - self.n + 1) : time_index + 1]\n",
    "                row = F.pad(row, (self.n - row.shape[1], 0), value=0)\n",
    "                new_x[:, time_index] = row\n",
    "            x = new_x\n",
    "\n",
    "        return self.run_calculations(x)\n",
    "\n",
    "    @abstractmethod\n",
    "    def loss_func(self, y_true, y_pred):\n",
    "        # to be implemented in child classes\n",
    "        pass\n",
    "   \n",
    "    def loss(self, logits, targets):\n",
    "        B, T, C = logits.shape\n",
    "        logits_flat = logits.view(B * T, C)\n",
    "        loss = self.loss_func(targets.view(B * T), logits_flat)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️⚠️⚠️ Define the model here. ⚠️⚠️⚠️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(BaseNGram):\n",
    "    def __init__(self, vocab_size, n):\n",
    "        super().__init__(vocab_size, n)\n",
    "        self.n = n\n",
    "        embedding_size = ... # ⚠️ Set this to 50\n",
    "        intermediate_size = ... # ⚠️ Set this to 120\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n",
    "        # ⚠️ Create fc layer with input size of embedding_size * n and output size of intermediate_size\n",
    "        self.fc = nn.Linear()\n",
    "        # ⚠️ 20% dropout\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.final = nn.Linear(intermediate_size, vocab_size)\n",
    "    \n",
    "    def run_calculations(self, x):\n",
    "        # ⚠️ pass x through the embedding layer\n",
    "\n",
    "        # Flatten the input (already done for you)\n",
    "        x = x.view(x.shape[0], x.shape[1], -1)\n",
    "\n",
    "        # ⚠️ Pass x through the rest of the layers\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def loss_func(self, y_true, y_pred):\n",
    "        # ⚠️ Use cross entropy loss\n",
    "        return loss\n",
    "    \n",
    "    def generate(self, x, max_len_new, temperature=1.0):\n",
    "        for _ in range(max_len_new):\n",
    "            logits = self(x, True)[:, -1] / temperature\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            x = torch.cat([x, next_token], dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚠️ Instantiate the model\n",
    "model = ...\n",
    "\n",
    "\n",
    "summary(\n",
    "    model,\n",
    "    input_data=[torch.zeros((256, 10), dtype=torch.long, device=device), True],\n",
    "    verbose=2,\n",
    "    device=device,\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"mult_adds\"],\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)\n",
    "# logits, loss = model(xb, yb)\n",
    "logits = model(xb)\n",
    "loss = model.loss(logits, yb)\n",
    "print('logits:', logits.shape)\n",
    "print('loss:', loss)\n",
    "\n",
    "print(decode(model.generate(xb, 10)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_dot(model(xb, True), params=dict(model.named_parameters()), show_attrs=False, show_saved=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.01)\n",
    "batch_size = 256\n",
    "for step in range(1000):\n",
    "    xb, yb = get_batch('train')\n",
    "    # logits, loss = model(xb, yb)\n",
    "    logits = model(xb)\n",
    "    loss = model.loss(logits, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (step + 1) % 50 == 0 or step == 0:\n",
    "        print(f'step: {step + 1}, loss: {loss.item():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(starting_text, max_length=200, temperature=0.9):\n",
    "    raw = model.generate(torch.tensor([encode(starting_text)], device=device), max_length, temperature)\n",
    "    return decode(raw[0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️⚠️⚠️ Print some generated text ⚠️⚠️⚠️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    # ⚠️ Insert starting text\n",
    "    print(generate_text(\"\", 200))\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️⚠️⚠️ Experiment Time: Try different values of `temperature` and `max_len` ⚠️⚠️⚠️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can do it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the end of the tutorial part. Now, try making changes on your own and see what happens. If you need help, just ask!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
